{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "berttest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1JYsrMAs5in",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54480003-1338-4b72-be78-10ee80054b37"
      },
      "source": [
        "!pip install transformers pandas google.colab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.4)\n",
            "Processing /root/.cache/pip/wheels/38/0d/59/701e300a337b2a2e07b27fe74dbfff0bc56ac58f711566ee67/google_colab-1.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: notebook~=5.2.0 in /usr/local/lib/python3.6/dist-packages (from google.colab) (5.2.2)\n",
            "Requirement already satisfied: six~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from google.colab) (1.12.0)\n",
            "Requirement already satisfied: google-auth~=1.4.0 in /usr/local/lib/python3.6/dist-packages (from google.colab) (1.4.2)\n",
            "Requirement already satisfied: portpicker~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google.colab) (1.2.0)\n",
            "Requirement already satisfied: tornado~=4.5.0 in /usr/local/lib/python3.6/dist-packages (from google.colab) (4.5.3)\n",
            "Requirement already satisfied: ipython~=5.5.0 in /usr/local/lib/python3.6/dist-packages (from google.colab) (5.5.0)\n",
            "Requirement already satisfied: ipykernel~=4.6.0 in /usr/local/lib/python3.6/dist-packages (from google.colab) (4.6.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (4.6.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (5.0.7)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (5.3.4)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (4.3.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (0.8.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook~=5.2.0->google.colab) (2.11.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth~=1.4.0->google.colab) (0.2.8)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth~=1.4.0->google.colab) (4.1.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth~=1.4.0->google.colab) (4.6)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (47.3.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (2.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython~=5.5.0->google.colab) (4.4.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook~=5.2.0->google.colab) (2.6.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook~=5.2.0->google.colab) (19.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook~=5.2.0->google.colab) (1.4.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook~=5.2.0->google.colab) (3.1.5)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook~=5.2.0->google.colab) (0.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook~=5.2.0->google.colab) (1.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=1.4.0->google.colab) (0.4.8)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython~=5.5.0->google.colab) (0.2.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook~=5.2.0->google.colab) (0.5.1)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.24.0, but you'll have pandas 1.0.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google.colab\n",
            "Successfully installed google.colab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBi6gMTy4Hp5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "83c7021b-1e21-4445-d8a1-c24eda13cb13"
      },
      "source": [
        "import pandas as pd\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "spreadsheet_name = ['huel_forum_posts',\n",
        "                    'bnz_forum_posts',\n",
        "                    'airline_forum_posts',\n",
        "                    'quickfile_forum_posts',\n",
        "                    'codecombat_forum_posts',\n",
        "                    'schizophrenia_forum_posts',\n",
        "                    'folksy_forum_posts']\n",
        "\n",
        "def loadgsheet(spreadsheet_name):\n",
        "  spreadsheet=pd.read_excel(f'gdrive/My Drive/bert_data/{spreadsheet_name}.xlsx')\n",
        "  spreadsheet.columns = ['post_text', 'post_id', 'user_id', 'username', 'reply_to_post_num', 'topic_id', 'post_num',\n",
        "             'reply_count', 'created_at', 'updated_at', 'num_reads', 'topic_slug', 'forum_name']\n",
        "  return spreadsheet[:2000]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppZos8TXY_xC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab3374fc-75ae-4748-b0d7-74fe297dfe0e"
      },
      "source": [
        "df_all = pd.DataFrame()\n",
        "for sheet_name in spreadsheet_name:\n",
        "  print(f'Joining {sheet_name}')\n",
        "  df = pd.read_excel(f'gdrive/My Drive/bert_data/{sheet_name}.xlsx')\n",
        "  df.columns = ['post_text', 'post_id', 'user_id', 'username', 'reply_to_post_num', 'topic_id', 'post_num',\n",
        "             'reply_count', 'created_at', 'updated_at', 'num_reads', 'topic_slug', 'forum_name']\n",
        "  print(df.head(2))\n",
        "  df_all = pd.concat([df_all, df]).dropna(axis=0)\n",
        "df_all.head(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Joining huel_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       post_text  ...  forum_name\n",
            "0  Purchased the trial pack, and used all flavors combining with Chocolate Huel Black Edition. Anyone that tried Berry with the Vanilla Huel? Share your ratings, vote your top 5 favorites ![:grin:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/grin.png?v=9)  * Vanilla  * Salted Caramel  * Banana  * Strawberry  * Apple Cinnamon  * Chocolate  * Peanut Butter  * Gingerbread  * Mocha  * Chocolate Cherry  * Mint-Chocolate  * Pumpkin Spice  * Berry 0 voters **Apple Cinnamon** \\- Excellent (5/5)  **Chocolate** \\- Double chocolate (5/5  **Pumpkin Spice** \\- Very Good (4/5)  **Peanut Butter** \\- Very Good (4/5)  **Vanilla** \\- Very Good (4/5)  **Mint Chocolate** \\- Not bad (3/5)  **Salted Caramel** \\- Not Bad (3/5)  **Mocha** \\- Not much of a difference to me (3/5)  **Strawberry** \\- Not Bad (2/5)  **Banana** \\- Not Bad (2/5)  **Berry Flavor** \\- Horrible(1/5)   ...  huel      \n",
            "1  i like mocha the best but it fks wth my stomach acid ![:face_with_hand_over_mouth:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/face_with_hand_over_mouth.png?v=9) berry and banana i like too apple cinamon amazng                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...  huel      \n",
            "\n",
            "[2 rows x 13 columns]\n",
            "Joining bnz_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                post_text  ...  forum_name\n",
            "0  Today is **‘Safer Internet Day’** , which is celebrated globally in February each year. It’s a day dedicated to promoting a safer online world. Netsafe are the official organising committee for this in New Zealand, and BNZ are proud to support them to encourage conversations about what a safer internet could look like. At BNZ, we think it’s an important day to recognise given that if you’re not staying safe online, you’re more likely to become a target of financial crime.     Netsafe have shared some great tips and resources, including online safety conversation starters for kids (under ten year olds), tweens (10-13 year olds) and teens (14-18 year olds). They recommend having regular chats with your young ones to educate them about online safety, and ultimately help minimise the damage if things do go wrong online. The types of questions they suggest asking your children are:  * What are some things you shouldn’t share online?  * What is the difference between public and private posts?  * Do you know what privacy settings are and what they are used for?    On top of this, Netsafe have a ‘Parents Toolkit’, as well as guides for checking privacy settings for some of the major social media channels – Facebook, Twitter, Instagram and Snapchat.     To read these resources in detail, and even print them off to take home so you can ask your kids, visit the following page <https://www.netsafe.org.nz/safer- internet-day/>     [![FB & IG Proudly supporting 2](https://aws1.discourse- cdn.com/bnz/optimized/2X/d/daa286b1727bf23324a80964f2bcf67079936775_2_500x500.jpeg) FB &amp; IG Proudly supporting 21200×1200 519 KB ](https://aws1.discourse- cdn.com/bnz/original/2X/d/daa286b1727bf23324a80964f2bcf67079936775.jpeg \"FB &amp; IG Proudly supporting 2\")   ...  bnz       \n",
            "1  When I go online to set a end date on a automatic payment it will not allow a date to be entered forcing me to call to get it done.  I was told it’s a known issue but is there any ideas on when the issue will be fixed, it’s been months and months since the new internet banking was rolled out and it’s never worked on this system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...  bnz       \n",
            "\n",
            "[2 rows x 13 columns]\n",
            "Joining airline_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   post_text  ...  forum_name\n",
            "0  Hey all - April credited 96 trips (~ 84 hours) of pay with 21 days off. Definitely much improved on loads this month, seeing quite a few full to the new limited capacity (~67%). It was rare to see less than 40 people per flight by the end of the month and noticed a LOT more people in the airport terminals. Hoping to see this trend continue!  MCO-HOU (DH) HOU-MAF-LAS-PHX-MCO (1st leg DH) OFF OFF OFF OFF OFF OFF OFF OFF OFF OFF OFF MCO-DEN-STL STL-BWI-ISP-BWI-JAX JAX-HOU-ATL-GSP GSP-ATL-MDW-MCO OFF OFF OFF OFF OFF OFF OFF MCO-DEN-STL STL-BWI-ISP-BWI-JAX JAX-HOU-ATL-GSP GSP-ATL-MDW-MCO OFF OFF OFF  ...  airline   \n",
            "1  It’s great to see a rebound, even if it’s small. Have you heard of any plans for Southwest to expand routes? I would think that with some of the legacies plan to reduce their routes, it might be a good time for airlines like LUv and JBLU to pick up market share strategically (without taking on too much additional variable costs).                                                                                                                                                                                                                                                                                ...  airline   \n",
            "\n",
            "[2 rows x 13 columns]\n",
            "Joining quickfile_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          post_text  ...  forum_name\n",
            "0  HI, Just disconnected from Santander to change to Starling and it isn’t listed?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...  quickfile \n",
            "1  Hi [@daveh001](/u/daveh001) Are you trying to connect this from “Open Banking” by any chance? The Starling feed isn’t actually Open Banking (although I believe this will change in the future). As long as you see the Starling logo on your account in QuickFile, you can go to the bank statement view, select **More Options** >> **Activate Bank Feed**. If you don’t see the Starling logo, select the account you’re trying to link up, as above, go to the ban statement view, **More Options** >> **Settings** , and set it to Starling. Both the logo and the option to activate the feed should then appear for you.   ...  quickfile \n",
            "\n",
            "[2 rows x 13 columns]\n",
            "Joining codecombat_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            post_text  ...  forum_name\n",
            "0  Hey guys I’m in a hurry here I have more things to do so i’m just gonna post the code here.// This function allows to fight until the certain time// and report about defeated enemies.function fightAndReport(untilTime) {    var defeated = 0;    while (true) {        var enemy = hero.findNearestEnemy();        if (enemy) {            hero.attack(enemy);            if (enemy.health &lt;= 0) {                defeated += 1;            }        }        if (hero.time &gt; untilTime) {            break;        }    }    hero.moveXY(59, 33);    hero.say(defeated);}// Fight 15 seconds and tell Naria how many enemies you defeated.fightAndReport(15);// Collect coins until the clock reaches 30 seconds.var item = hero.findNearestItem();if (hero.time &gt; 30) {    hero.moveXY(item.pos.x,item.pos.y);}// Tell Naria how much gold you collected.hero.say(hero.gold);// Fight enemies until the clock reaches 45 seconds.fightAndReport(45);  ...  codecombat\n",
            "1  athian:ifInstead of if put while athian:var item = hero.findNearestItem();This put inside the while loop under it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...  codecombat\n",
            "\n",
            "[2 rows x 13 columns]\n",
            "Joining schizophrenia_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             post_text  ...     forum_name\n",
            "0  ![](http://www.peteearley.com/wp-content/themes/education/images/favicon.ico) [Pete Earley – 8 Jun 20](http://www.peteearley.com/2020/06/08/federal-govt- accused-of-abandoning-research-that-would-provide-short-term-help-to-the-most- seriously-mentally- ill/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A%2Bpeteearley%2B%28The%2BOfficial%2BBlog%2Bof%2BAuthor%2BPete%2BEarley%29 \"12:00PM - 08 June 2020\") ![](https://aws1.discourse- cdn.com/schizophrenia/uploads/default/original/3X/f/2/f2e8cca16cf15cebbe4ee807f64c29a9dcc09d25.jpeg) ### [Federal Govt. Accused Of Abandoning Research That Would Provide Short Term...](http://www.peteearley.com/2020/06/08/federal-govt-accused-of- abandoning-research-that-would-provide-short-term-help-to-the-most-seriously- mentally- ill/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A%2Bpeteearley%2B%28The%2BOfficial%2BBlog%2Bof%2BAuthor%2BPete%2BEarley%29) Dr. E. Fuller Torrey rips into NIMH, its advisory board and NAMI (6-8-20) Dr. E. Fuller Torrey is again accusing the National Institutes of Mental Health of virtually abandoning clinical trials that could help Americans with schizophrenia and bipolar...   ...  schizophrenia\n",
            "1  I’m taking my first capsule of Lumateperone 42 mg tonight. Steady state should be reached in 5 days. Will share my experiences here.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ...  schizophrenia\n",
            "\n",
            "[2 rows x 13 columns]\n",
            "Joining folksy_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    post_text  ...  forum_name\n",
            "0  Im a little confused not difficult at the moment img src https sjc5 discourse-cdn com folksy images emoji twitter rofl png v 9 title rofl class emoji alt rofl Ive just been browsing the home page and decided to look in the Fathers Day cards, the heading says Fathers day is the 16th June which isnt even a Sunday but on my calendar its marked at 21st June Has the world gone completely crazy or have a class mention-group href groups admins admins a just not updated last years header img src https sjc5 discourse-cdn com folksy images emoji twitter slight smile png v 9 title slight smile class emoji alt slight smile  ...  folksy    \n",
            "1  I had to look it up too and the date I got was 21 June too                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ...  folksy    \n",
            "\n",
            "[2 rows x 13 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>post_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>username</th>\n",
              "      <th>reply_to_post_num</th>\n",
              "      <th>topic_id</th>\n",
              "      <th>post_num</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>created_at</th>\n",
              "      <th>updated_at</th>\n",
              "      <th>num_reads</th>\n",
              "      <th>topic_slug</th>\n",
              "      <th>forum_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Interesting! Here are my ratings (I haven’t tried them all yet). **Mint Chocolate** \\- 5/5. To me this tastes exactly like what you’d expect a good chocolate mint to taste like.  **Chocolate Cherry** \\- 4.5/5. I end up using a little more than the serving size because I always want it to have more cherry flavor. Otherwise, really good.  **Apple Cinnamon** \\- 4/5. To me this taste like apple pie, which is great. I could rate it 5/5, except that because it’s so sweet, I feel like I need to cycle it with other flavors. If this was my only flavor, I’d probably get sick of it.  **Gingerbread** \\- 3/5. It’s another one I wish was a little stronger. I’ve always liked gingerbread a little more gingery too, but I’ve enjoyed trying this out and mixing it into the rotation.  **Salted Caramel** \\- 3/5. It’s totally fine, but this was just never going to be my favorite flavor. Also, I think I’ve realized I’d rather get the extra salt somewhere else in my diet. I would expect people who like salted caramel to mostly be happy with this flavor.  **Berry** \\- 2.5/5. To me it tastes more like a sugary kids cereal kind of berry than actual berries. I add a couple drops of orange extract every time now, and it tastes much more like actual fruit (unsurprisingly). I’d probably rate that combo 4/5.</td>\n",
              "      <td>25677</td>\n",
              "      <td>4635</td>\n",
              "      <td>Tom</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8815</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2020-06-04T17:03:37.547Z</td>\n",
              "      <td>2020-06-04T17:04:06.818Z</td>\n",
              "      <td>14</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i tried choc cherry only one i didnt like</td>\n",
              "      <td>25678</td>\n",
              "      <td>4856</td>\n",
              "      <td>matt009</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8815</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-06-04T18:19:22.386Z</td>\n",
              "      <td>2020-06-04T18:19:22.386Z</td>\n",
              "      <td>14</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I’ll be ordering one bag of Vanilla Black Edition next shipment - but for now I use the Vanilla boost in Chocolate Huel B. E. and it goes great together, Vanilla chocolately flavor. I also use Vanilla Almond Milk, so that might help the Vanilla flavor too.</td>\n",
              "      <td>25680</td>\n",
              "      <td>4859</td>\n",
              "      <td>Justin_Keikhlasan</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8815</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-06-04T20:01:55.915Z</td>\n",
              "      <td>2020-06-04T20:01:55.915Z</td>\n",
              "      <td>13</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>My next order is bringing mint chocolate, I’m pretty excited to try it!</td>\n",
              "      <td>25683</td>\n",
              "      <td>2758</td>\n",
              "      <td>DM87</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8815</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-06-04T22:18:10.521Z</td>\n",
              "      <td>2020-06-04T22:18:10.521Z</td>\n",
              "      <td>13</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Thanks. How much booster do you need to use, or does it vary?</td>\n",
              "      <td>25729</td>\n",
              "      <td>34</td>\n",
              "      <td>Desert_Way</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8815</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>2020-06-07T14:32:13.890Z</td>\n",
              "      <td>2020-06-07T14:32:13.890Z</td>\n",
              "      <td>9</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             post_text  ... forum_name\n",
              "2    Interesting! Here are my ratings (I haven’t tried them all yet). **Mint Chocolate** \\- 5/5. To me this tastes exactly like what you’d expect a good chocolate mint to taste like.  **Chocolate Cherry** \\- 4.5/5. I end up using a little more than the serving size because I always want it to have more cherry flavor. Otherwise, really good.  **Apple Cinnamon** \\- 4/5. To me this taste like apple pie, which is great. I could rate it 5/5, except that because it’s so sweet, I feel like I need to cycle it with other flavors. If this was my only flavor, I’d probably get sick of it.  **Gingerbread** \\- 3/5. It’s another one I wish was a little stronger. I’ve always liked gingerbread a little more gingery too, but I’ve enjoyed trying this out and mixing it into the rotation.  **Salted Caramel** \\- 3/5. It’s totally fine, but this was just never going to be my favorite flavor. Also, I think I’ve realized I’d rather get the extra salt somewhere else in my diet. I would expect people who like salted caramel to mostly be happy with this flavor.  **Berry** \\- 2.5/5. To me it tastes more like a sugary kids cereal kind of berry than actual berries. I add a couple drops of orange extract every time now, and it tastes much more like actual fruit (unsurprisingly). I’d probably rate that combo 4/5.   ...  huel     \n",
              "3   i tried choc cherry only one i didnt like                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ...  huel     \n",
              "5    I’ll be ordering one bag of Vanilla Black Edition next shipment - but for now I use the Vanilla boost in Chocolate Huel B. E. and it goes great together, Vanilla chocolately flavor. I also use Vanilla Almond Milk, so that might help the Vanilla flavor too.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...  huel     \n",
              "6    My next order is bringing mint chocolate, I’m pretty excited to try it!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ...  huel     \n",
              "10   Thanks. How much booster do you need to use, or does it vary?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...  huel     \n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7ngBhDz0Jpt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "cb68e580-7e88-409b-f4dd-79f2fc66725b"
      },
      "source": [
        "# Shuffle df\n",
        "df_all = df_all.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Sample df\n",
        "df_all[['post_text', 'forum_name']].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>forum_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>84376</th>\n",
              "      <td>Hi [@tom_44](/u/tom_44) Although the invoice isn’t responsive, providing it’s enabled in your settings, we will show an skimmed down invoice for mobile users. There’s more info on this here: [New Mobile Friendly Invoice Design](https://community.quickfile.co.uk/t/new-mobile-friendly-invoice- design/10616)</td>\n",
              "      <td>quickfile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23592</th>\n",
              "      <td>My hero says “Fire” and the coordinates “62,51”. The 3 enemies are on the screen already.</td>\n",
              "      <td>codecombat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29473</th>\n",
              "      <td>You have several things so I’m numbering them to keep them separate.soldiers vs witchesI’d guess that has to do with witch AoE damage, with all your soldiers in one group both witches are attacking and hitting the same group of soldiers instead of hitting different groups, killing them faster. (Sometimes small difference can tip the balance from win to lose)hero vs friend vision:Assuming you can see everyone on the screen (which glasses?), if you are using friend.distanceTo to choose the closest target from the list then, I’d agree that it shouldn’t matter if it is your list or their list. If you aren’t using friend distanceTo then they are attacking the first one on the list that matches…code auto complete:The auto complete does a simple/dumb match and replace: first it matches on findEnemies and then “replaces” what you are typing with  this.findEnemies() (language specific). To make the change it would need to replace check against “*.findEnemies()” and at the same time know what the default replacement is for your language “this.” if there isn’t a leading “blah.” (know that “this/self/(/etc” is replace-checked with a ‘*’ instead of the actual characters). Handle the different languages and future ones, when/if any come along.</td>\n",
              "      <td>codecombat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83046</th>\n",
              "      <td>&amp;gt; def commandSoldiers():&amp;gt;     for soldier in hero.findFriends():&amp;gt;         enemy = soldier.findNearestEnemy()&amp;gt;         if enemy:&amp;gt;             hero.command(soldier, \"attack\", enemy)代码如上，报错部分是hero.command()里的enemy，为什么呀～求教WechatIMG26.jpeg1064×254 68 KB</td>\n",
              "      <td>codecombat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84973</th>\n",
              "      <td>I like your ear rings</td>\n",
              "      <td>schizophrenia</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               post_text     forum_name\n",
              "84376  Hi [@tom_44](/u/tom_44) Although the invoice isn’t responsive, providing it’s enabled in your settings, we will show an skimmed down invoice for mobile users. There’s more info on this here: [New Mobile Friendly Invoice Design](https://community.quickfile.co.uk/t/new-mobile-friendly-invoice- design/10616)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 quickfile    \n",
              "23592  My hero says “Fire” and the coordinates “62,51”. The 3 enemies are on the screen already.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          codecombat   \n",
              "29473  You have several things so I’m numbering them to keep them separate.soldiers vs witchesI’d guess that has to do with witch AoE damage, with all your soldiers in one group both witches are attacking and hitting the same group of soldiers instead of hitting different groups, killing them faster. (Sometimes small difference can tip the balance from win to lose)hero vs friend vision:Assuming you can see everyone on the screen (which glasses?), if you are using friend.distanceTo to choose the closest target from the list then, I’d agree that it shouldn’t matter if it is your list or their list. If you aren’t using friend distanceTo then they are attacking the first one on the list that matches…code auto complete:The auto complete does a simple/dumb match and replace: first it matches on findEnemies and then “replaces” what you are typing with  this.findEnemies() (language specific). To make the change it would need to replace check against “*.findEnemies()” and at the same time know what the default replacement is for your language “this.” if there isn’t a leading “blah.” (know that “this/self/(/etc” is replace-checked with a ‘*’ instead of the actual characters). Handle the different languages and future ones, when/if any come along.  codecombat   \n",
              "83046  &gt; def commandSoldiers():&gt;     for soldier in hero.findFriends():&gt;         enemy = soldier.findNearestEnemy()&gt;         if enemy:&gt;             hero.command(soldier, \"attack\", enemy)代码如上，报错部分是hero.command()里的enemy，为什么呀～求教WechatIMG26.jpeg1064×254 68 KB                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            codecombat   \n",
              "84973  I like your ear rings                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              schizophrenia"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4SZ-DOOEbt9",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Data Cleansing and Prep\n",
        "Now we've loaded the data, we must remove noise from the dataset. Please explore some techniques in which we could clean the data in order to us to see how well pre-trained BERT works on our dataset. Luckily, due to the way BERT tokenises the data, we don't need to the same extent of data preprocessing as required of previous NLP models. However we still need to -\n",
        "\n",
        "1. Filter nulls\n",
        "2. Filter for duplicates\n",
        "3. [Optional] Remove post_text which does not have vocab in pre-trained BERT. 4. Later, we will leave this in for finetuning.\n",
        "5. Hyperlinks\n",
        "6. Foreign languages - there are multilingual BERT models\n",
        "\n",
        "Any more you can think of?\n",
        "Encode the labels - map categorical labels to numerical values\n",
        "See here for Pandas cleaning tutorials\n",
        "See here for beginner EDA tutorial for NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxcETy6tEevJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "b8eae647-b316-45b0-b00c-245525c6d9c3"
      },
      "source": [
        "df_all.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "post_text            object \n",
              "post_id              object \n",
              "user_id              int64  \n",
              "username             object \n",
              "reply_to_post_num    float64\n",
              "topic_id             int64  \n",
              "post_num             int64  \n",
              "reply_count          int64  \n",
              "created_at           object \n",
              "updated_at           object \n",
              "num_reads            object \n",
              "topic_slug           object \n",
              "forum_name           object \n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKt45gM9N--n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "7436ed97-c494-497e-c952-7d33526957f3"
      },
      "source": [
        "df_all.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "post_text            0\n",
              "post_id              0\n",
              "user_id              0\n",
              "username             0\n",
              "reply_to_post_num    0\n",
              "topic_id             0\n",
              "post_num             0\n",
              "reply_count          0\n",
              "created_at           0\n",
              "updated_at           0\n",
              "num_reads            0\n",
              "topic_slug           0\n",
              "forum_name           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFyhjI-Bj4jY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "df_all.drop_duplicates\n",
        "df_all['post_text'] = df_all['post_text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvCBS8NeG8xL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers as ppb # pytorch transformers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQtfguvdjI5f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f48a5e0-4554-4d8e-9987-7658c824b5eb"
      },
      "source": [
        "df_all.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97621, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWGrqJtusCSn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "c7c4d767-7541-4119-85cd-0afec2f463ee"
      },
      "source": [
        "# Shuffle df\n",
        "df_all = df_all.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Sample df\n",
        "df_all[['post_text', 'forum_name']].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>forum_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10553</th>\n",
              "      <td>Good luck with your stall Dawn a class mention href u hobbitgirlie1880 hobbitgirlie1880 a \\n Thank you Deborah for doing the thread last month and this month Max x</td>\n",
              "      <td>folksy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14647</th>\n",
              "      <td>raising deads on zero sum with commands likehero.command(friend, \"attack\", friend.findNearest(enemies))will result in errors sayingcommand's argument minion should have type unit, but got object: Acoldan. Hero Placeholder 1 (team ogres) can't command Acoldan (team humans)removing the raise dead functions will make the codes run perfectly. This errors only shows when a raise dead was accomplished beforehand (as far as i know). The Acoldan is always different with every run, its most likely the name of the enemy units.Here’s my code snipwhile True:    enemies = hero.findEnemies()    nearestEnemy = hero.findNearest(enemies)        enemyHero = hero.findNearest(hero.findByType(\"sorcerer\",enemies))        friends = hero.findFriends()    # In case theres a python error from codecombat    friends.remove(hero.findByType(\"cage\"))    friends.remove(hero.findByType(\"yeti\"))    while True:        if nearestEnemy:            if hero.distanceTo(nearestEnemy) &amp;lt; 20 and hero.isReady(\"mana-blast\"):                hero.manaBlast()                summon(\"soldier\",5)            elif len(hero.findByType(\"griffin-rider\")) &amp;lt; 3:                if summon(\"griffin-rider\") == False:                    break            else:                break        else:            break        if len(hero.findByType(\"artillery\")) &amp;lt; 1:        summon(\"artillery\")    for friend in friends:        cage = hero.findByType(\"cage\")        # Please don't mess with the cage        if cage:            enemies.remove(cage[0])        nearest_to_friends = friend.findNearest(enemies)        hero.command(friend, \"attack\", nearest_to_friends)        # Raise Dead Corpses    if hero.isReady(\"raise-dead\"):        corpses = hero.findCorpses()        if len(corpses) &amp;gt;= 1:            undead_army = []            for corpse in corpses:                if hero.distanceTo(corpse) &amp;lt; 20:                    undead_army.append(corpse)                if len(undead_army) &amp;gt;= 5:                    hero.cast(\"raise-dead\")                    break</td>\n",
              "      <td>codecombat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28546</th>\n",
              "      <td>Of course! If your accountant doesn’t have access to any other QuickFile accounts, you can add them by going to **Account Settings** &gt;&gt; **Team Management** , and even customise what areas they can and can’t access. However, you do need to be the account administrator to set this up. If they access several accounts, then they will need to use an [Affinity](http://community.quickfile.co.uk/c/knowledgebase/affinity) account. There is a small fee for this, but it gives them greater flexibility when it comes to accessing QuickFile accounts (such as white labeling, key date overview etc.)</td>\n",
              "      <td>quickfile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90290</th>\n",
              "      <td>Yes - we’ll take a look at it. But to keep the details of your account etc. private, we’ve asked for them in the private message.</td>\n",
              "      <td>quickfile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5203</th>\n",
              "      <td>You should, yes. Although the description is likely to be different, depending on where the untagged entry came from. At the top of the invoice, if you click the view all the payments: ![image](</td>\n",
              "      <td>quickfile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   post_text  forum_name\n",
              "10553  Good luck with your stall Dawn a class mention href u hobbitgirlie1880 hobbitgirlie1880 a \\n Thank you Deborah for doing the thread last month and this month Max x                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    folksy    \n",
              "14647  raising deads on zero sum with commands likehero.command(friend, \"attack\", friend.findNearest(enemies))will result in errors sayingcommand's argument minion should have type unit, but got object: Acoldan. Hero Placeholder 1 (team ogres) can't command Acoldan (team humans)removing the raise dead functions will make the codes run perfectly. This errors only shows when a raise dead was accomplished beforehand (as far as i know). The Acoldan is always different with every run, its most likely the name of the enemy units.Here’s my code snipwhile True:    enemies = hero.findEnemies()    nearestEnemy = hero.findNearest(enemies)        enemyHero = hero.findNearest(hero.findByType(\"sorcerer\",enemies))        friends = hero.findFriends()    # In case theres a python error from codecombat    friends.remove(hero.findByType(\"cage\"))    friends.remove(hero.findByType(\"yeti\"))    while True:        if nearestEnemy:            if hero.distanceTo(nearestEnemy) &lt; 20 and hero.isReady(\"mana-blast\"):                hero.manaBlast()                summon(\"soldier\",5)            elif len(hero.findByType(\"griffin-rider\")) &lt; 3:                if summon(\"griffin-rider\") == False:                    break            else:                break        else:            break        if len(hero.findByType(\"artillery\")) &lt; 1:        summon(\"artillery\")    for friend in friends:        cage = hero.findByType(\"cage\")        # Please don't mess with the cage        if cage:            enemies.remove(cage[0])        nearest_to_friends = friend.findNearest(enemies)        hero.command(friend, \"attack\", nearest_to_friends)        # Raise Dead Corpses    if hero.isReady(\"raise-dead\"):        corpses = hero.findCorpses()        if len(corpses) &gt;= 1:            undead_army = []            for corpse in corpses:                if hero.distanceTo(corpse) &lt; 20:                    undead_army.append(corpse)                if len(undead_army) &gt;= 5:                    hero.cast(\"raise-dead\")                    break  codecombat\n",
              "28546  Of course! If your accountant doesn’t have access to any other QuickFile accounts, you can add them by going to **Account Settings** >> **Team Management** , and even customise what areas they can and can’t access. However, you do need to be the account administrator to set this up. If they access several accounts, then they will need to use an [Affinity](http://community.quickfile.co.uk/c/knowledgebase/affinity) account. There is a small fee for this, but it gives them greater flexibility when it comes to accessing QuickFile accounts (such as white labeling, key date overview etc.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          quickfile \n",
              "90290  Yes - we’ll take a look at it. But to keep the details of your account etc. private, we’ve asked for them in the private message.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      quickfile \n",
              "5203   You should, yes. Although the description is likely to be different, depending on where the untagged entry came from. At the top of the invoice, if you click the view all the payments: ![image](                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     quickfile "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvwKwRYNsOz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "753cabd8-0c3b-41f1-fa93-d6a838fc55c7"
      },
      "source": [
        "df_all['forum_name'] = df_all['forum_name'].astype('category')\n",
        "df_all['forum_name_encoded'] = df_all['forum_name'].cat.codes.astype('int32')\n",
        "df_all.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>post_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>username</th>\n",
              "      <th>reply_to_post_num</th>\n",
              "      <th>topic_id</th>\n",
              "      <th>post_num</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>created_at</th>\n",
              "      <th>updated_at</th>\n",
              "      <th>num_reads</th>\n",
              "      <th>topic_slug</th>\n",
              "      <th>forum_name</th>\n",
              "      <th>forum_name_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>88129</th>\n",
              "      <td>I’m sorry to hear that. My dad has always been violent and when we were kids he would go into fits of rage and scream at the top of his lungs. He never was personable or approachable and still to this day, he’s like that, despite the fact that he’s 70 years old now.</td>\n",
              "      <td>2202224</td>\n",
              "      <td>14221</td>\n",
              "      <td>Solidus</td>\n",
              "      <td>2.0</td>\n",
              "      <td>196413</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-05-03T22:38:39.450Z</td>\n",
              "      <td>2020-05-03T22:38:39.450Z</td>\n",
              "      <td>22</td>\n",
              "      <td>my-dads-boss-told-him-that-he-should-beat-us-kids</td>\n",
              "      <td>schizophrenia</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85769</th>\n",
              "      <td>![](</td>\n",
              "      <td>33241</td>\n",
              "      <td>102</td>\n",
              "      <td>ForceOne</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1645</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2015-10-28T21:18:32.041Z</td>\n",
              "      <td>2015-10-28T21:18:32.041Z</td>\n",
              "      <td>23</td>\n",
              "      <td>whmcs-integration</td>\n",
              "      <td>quickfile</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50780</th>\n",
              "      <td>Jeff, I think you answered your own question there. Unless you have a significant other, pet sitter or some other plan in place, it would be very difficult to own a pet. Now Chia Pets on the other hand are perfect for this job. Chris</td>\n",
              "      <td>17813</td>\n",
              "      <td>3</td>\n",
              "      <td>Chris</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6638</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2017-08-15T15:11:33.000Z</td>\n",
              "      <td>2017-08-15T15:11:42.349Z</td>\n",
              "      <td>126</td>\n",
              "      <td>can-you-be-a-pilot-and-own-a-pet</td>\n",
              "      <td>airline</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64898</th>\n",
              "      <td>![](</td>\n",
              "      <td>33436</td>\n",
              "      <td>2</td>\n",
              "      <td>Glenn</td>\n",
              "      <td>11.0</td>\n",
              "      <td>9297</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>2015-11-02T16:42:43.506Z</td>\n",
              "      <td>2015-11-02T16:42:43.506Z</td>\n",
              "      <td>34</td>\n",
              "      <td>employees-expenses</td>\n",
              "      <td>quickfile</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25843</th>\n",
              "      <td>Andrew, Out of curiosity, what college is that?  I am always leery of part 141schools, I find that the extra regulations they place on flight training often drive the costs way up compared to part 61 training. Chris</td>\n",
              "      <td>3164</td>\n",
              "      <td>3</td>\n",
              "      <td>Chris</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1355</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-10-26T21:43:22.335Z</td>\n",
              "      <td>2016-10-26T21:43:22.335Z</td>\n",
              "      <td>82</td>\n",
              "      <td>atp-out-of-hs</td>\n",
              "      <td>airline</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                         post_text  ... forum_name_encoded\n",
              "88129  I’m sorry to hear that. My dad has always been violent and when we were kids he would go into fits of rage and scream at the top of his lungs. He never was personable or approachable and still to this day, he’s like that, despite the fact that he’s 70 years old now.   ...  6                \n",
              "85769  ![](                                                                                                                                                                                                                                                                         ...  5                \n",
              "50780  Jeff, I think you answered your own question there. Unless you have a significant other, pet sitter or some other plan in place, it would be very difficult to own a pet. Now Chia Pets on the other hand are perfect for this job. Chris                                    ...  0                \n",
              "64898  ![](                                                                                                                                                                                                                                                                         ...  5                \n",
              "25843  Andrew, Out of curiosity, what college is that?  I am always leery of part 141schools, I find that the extra regulations they place on flight training often drive the costs way up compared to part 61 training. Chris                                                      ...  0                \n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_cs4evqsYUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "312d700a-829f-4f08-8262-3b9e537c7214"
      },
      "source": [
        "!pip install html2text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: html2text in /usr/local/lib/python3.6/dist-packages (2020.1.16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1nz8GJZsc0_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "220ac42d-6e33-439f-c1b6-cd9701b4dfdd"
      },
      "source": [
        "import html2text\n",
        "h = html2text.HTML2Text()\n",
        "h.unicode_snob = True\n",
        "df_all['post_text'] = df_all['post_text'].apply((lambda x: h.handle(x).replace('\\n', ' ').replace('  ', ' ')))\n",
        "df_all[df_all['forum_name']=='folksy'].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>post_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>username</th>\n",
              "      <th>reply_to_post_num</th>\n",
              "      <th>topic_id</th>\n",
              "      <th>post_num</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>created_at</th>\n",
              "      <th>updated_at</th>\n",
              "      <th>num_reads</th>\n",
              "      <th>topic_slug</th>\n",
              "      <th>forum_name</th>\n",
              "      <th>forum_name_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>44895</th>\n",
              "      <td>Thanks for the advice I will give that ago, Pam</td>\n",
              "      <td>151282</td>\n",
              "      <td>3650</td>\n",
              "      <td>pamcodner</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9888</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-01-17T14:04:38.638Z</td>\n",
              "      <td>2016-01-17T14:04:38.638Z</td>\n",
              "      <td>117</td>\n",
              "      <td>poor-sales</td>\n",
              "      <td>folksy</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34258</th>\n",
              "      <td>Not as much fun img src https sjc5 discourse-cdn com folksy images emoji twitter frowning png v 5 title frowning class emoji alt frowning Ive only just registered so ill be in the same position next year I hope you sort yours out x</td>\n",
              "      <td>153372</td>\n",
              "      <td>2531</td>\n",
              "      <td>VioletJewellery</td>\n",
              "      <td>3.0</td>\n",
              "      <td>10069</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-01-23T13:12:03.157Z</td>\n",
              "      <td>2016-01-23T13:12:03.157Z</td>\n",
              "      <td>56</td>\n",
              "      <td>what-are-you-making-this-weekend</td>\n",
              "      <td>folksy</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33331</th>\n",
              "      <td>I get sales most days on etsy but do have over 300 items on there I dont use any form of adverts or social media for promoting, just because I really dont like it and havent really got the hang of it Everyone says with Folksy you have to drive your own traffic which isnt my cup of tea</td>\n",
              "      <td>334578</td>\n",
              "      <td>3118</td>\n",
              "      <td>ElizabethanRuby</td>\n",
              "      <td>4.0</td>\n",
              "      <td>17438</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2018-12-08T17:52:08.988Z</td>\n",
              "      <td>2018-12-08T17:52:08.988Z</td>\n",
              "      <td>48</td>\n",
              "      <td>how-do-i-make-an-invoice-out-to-a-customer</td>\n",
              "      <td>folksy</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11500</th>\n",
              "      <td>I just looked in your shop but it says you havent got anything listed</td>\n",
              "      <td>156034</td>\n",
              "      <td>2718</td>\n",
              "      <td>hobbitgirlie1880</td>\n",
              "      <td>6.0</td>\n",
              "      <td>10177</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-01-31T23:12:59.621Z</td>\n",
              "      <td>2016-01-31T23:12:59.621Z</td>\n",
              "      <td>76</td>\n",
              "      <td>promoting-you-on-instagram</td>\n",
              "      <td>folksy</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68842</th>\n",
              "      <td>Thanks Paul, Ill have a look at your Facebook page We have a scroll saw and Ive started to do small things like bookmarks with it, but Im not sure that Ive got the patience or confidence yet to try a whole chess set - maybe one for Paul, he is much better at this sort of thing than me</td>\n",
              "      <td>348027</td>\n",
              "      <td>8223</td>\n",
              "      <td>gorlech</td>\n",
              "      <td>14.0</td>\n",
              "      <td>17850</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>2019-03-13T21:02:16.157Z</td>\n",
              "      <td>2019-03-13T21:02:16.157Z</td>\n",
              "      <td>42</td>\n",
              "      <td>can-a-woodcraft-shop-succeed-on-folksy</td>\n",
              "      <td>folksy</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                            post_text  ... forum_name_encoded\n",
              "44895  Thanks for the advice I will give that ago, Pam                                                                                                                                                                                                                                                 ...  3                \n",
              "34258  Not as much fun img src https sjc5 discourse-cdn com folksy images emoji twitter frowning png v 5 title frowning class emoji alt frowning Ive only just registered so ill be in the same position next year I hope you sort yours out x                                                         ...  3                \n",
              "33331  I get sales most days on etsy but do have over 300 items on there I dont use any form of adverts or social media for promoting, just because I really dont like it and havent really got the hang of it Everyone says with Folksy you have to drive your own traffic which isnt my cup of tea   ...  3                \n",
              "11500  I just looked in your shop but it says you havent got anything listed                                                                                                                                                                                                                           ...  3                \n",
              "68842  Thanks Paul, Ill have a look at your Facebook page We have a scroll saw and Ive started to do small things like bookmarks with it, but Im not sure that Ive got the patience or confidence yet to try a whole chess set - maybe one for Paul, he is much better at this sort of thing than me   ...  3                \n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93Cl93gyrv04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df_all.post_text.values\n",
        "labels = df_all.forum_name_encoded.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdJy3Ar8stpZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2a602b9b-71dc-45e2-c7f6-0c10fcf08a7a"
      },
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "# Load pretrained DistilBERT tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Sample df\n",
        "df_all[['post_text', 'forum_name']].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>forum_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7934</th>\n",
              "      <td>Hello again, could you add 3D under Glass Art please</td>\n",
              "      <td>folksy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92468</th>\n",
              "      <td>Hi Mathew, I am using the older VAT submission method.</td>\n",
              "      <td>quickfile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66592</th>\n",
              "      <td>hey anyone else participating</td>\n",
              "      <td>codecombat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91853</th>\n",
              "      <td>Yes, I have the same problem. If I increase the number of loops it still attacks only 1 enemy…</td>\n",
              "      <td>codecombat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37375</th>\n",
              "      <td>Thank you! …</td>\n",
              "      <td>quickfile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                             post_text  forum_name\n",
              "7934   Hello again, could you add 3D under Glass Art please                                             folksy    \n",
              "92468  Hi Mathew, I am using the older VAT submission method.                                           quickfile \n",
              "66592  hey anyone else participating                                                                    codecombat\n",
              "91853  Yes, I have the same problem. If I increase the number of loops it still attacks only 1 enemy…   codecombat\n",
              "37375  Thank you! …                                                                                     quickfile "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6xnWfTgq_iQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "efde75fc-78b9-4bb4-d4bb-f66cf093297d"
      },
      "source": [
        "max_len = 0\n",
        "# For first 10 sentences - \n",
        "for s in sentences[:10]:\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(s, add_special_tokens=True)\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxsuvVeOs-F3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "91902098-1d5c-4577-cff4-15c781795e4c"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for s in sentences:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        s,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  this us my code# Build 4 decoys, then report the total to Naria.decoysBuilt = 0while True: coin = hero.findNearestItem() if coin: # Collect the coin! coin = hero.findNearestItem() hero.moveXY(coin.pos.x, coin.pos.y) pass # Each decoy costs 25 gold. # If hero.gold is greater than or equal to 25: if hero.gold >= 25: # buildXY a \"decoy\" hero.buildXY(\"decoy\", 53, 32) # Add 1 to the decoysBuilt count. if decoysBuilt == 4: # Break out of the loop when you have built 4. break pass hero.say(\"Done building decoys!\")hero.moveXY(14, 36)# Say how many decoys you built.hero.say(decoysBuilt) \n",
            "Token IDs: tensor([  101,  2023,  2149,  2026,  3642,  1001,  3857,  1018, 21933,  7274,\n",
            "         1010,  2059,  3189,  1996,  2561,  2000,  6583,  4360,  1012, 21933,\n",
            "         7274,  8569,  4014,  2102,  1027,  1014, 19927,  2995,  1024,  9226,\n",
            "         1027,  5394,  1012,  2424, 22084, 28533,  4221,  2213,  1006,  1007,\n",
            "         2065,  9226,  1024,  1001,  8145,  1996,  9226,   999,  9226,  1027,\n",
            "         5394,  1012,  2424, 22084, 28533,  4221,  2213,  1006,  1007,  5394,\n",
            "         1012,  2693, 18037,   102])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVwh2JN7taj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7e7bf884-57de-4151-af46-4df01eb89f61"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print(f'{train_size:>5,} training samples')\n",
        "print(f'{val_size:>5,} validation samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "87,858 training samples\n",
            "9,763 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1wI4Oa4thAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4LDrYSZtmm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0536d199-1c0d-456b-c359-60e9502afa27"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 7, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU. Make sure you enable the runtime clicking [Runtime]->[Change Runtime Type]->[Hardware Accelerator]->GPU->[Save]\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "129Kh1Xxp-d-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRM4oKGot4Be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MuHxuHDuBRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wz-Ipq5u9Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda = torch.device('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGPWoVbFuC-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhMijgO1uI3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5422eeb-81d7-4bc3-ba52-b799549a4265"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(cuda)\n",
        "        b_input_mask = batch[1].to(cuda)\n",
        "        b_labels = batch[2].to(cuda)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(cuda)\n",
        "        b_input_mask = batch[1].to(cuda)\n",
        "        b_labels = batch[2].to(cuda)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  2,746.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  2,746.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  2,746.    Elapsed: 0:00:44.\n",
            "  Batch   160  of  2,746.    Elapsed: 0:00:59.\n",
            "  Batch   200  of  2,746.    Elapsed: 0:01:14.\n",
            "  Batch   240  of  2,746.    Elapsed: 0:01:29.\n",
            "  Batch   280  of  2,746.    Elapsed: 0:01:44.\n",
            "  Batch   320  of  2,746.    Elapsed: 0:01:59.\n",
            "  Batch   360  of  2,746.    Elapsed: 0:02:14.\n",
            "  Batch   400  of  2,746.    Elapsed: 0:02:29.\n",
            "  Batch   440  of  2,746.    Elapsed: 0:02:44.\n",
            "  Batch   480  of  2,746.    Elapsed: 0:02:59.\n",
            "  Batch   520  of  2,746.    Elapsed: 0:03:14.\n",
            "  Batch   560  of  2,746.    Elapsed: 0:03:29.\n",
            "  Batch   600  of  2,746.    Elapsed: 0:03:44.\n",
            "  Batch   640  of  2,746.    Elapsed: 0:03:59.\n",
            "  Batch   680  of  2,746.    Elapsed: 0:04:14.\n",
            "  Batch   720  of  2,746.    Elapsed: 0:04:29.\n",
            "  Batch   760  of  2,746.    Elapsed: 0:04:44.\n",
            "  Batch   800  of  2,746.    Elapsed: 0:04:59.\n",
            "  Batch   840  of  2,746.    Elapsed: 0:05:14.\n",
            "  Batch   880  of  2,746.    Elapsed: 0:05:29.\n",
            "  Batch   920  of  2,746.    Elapsed: 0:05:44.\n",
            "  Batch   960  of  2,746.    Elapsed: 0:05:59.\n",
            "  Batch 1,000  of  2,746.    Elapsed: 0:06:14.\n",
            "  Batch 1,040  of  2,746.    Elapsed: 0:06:29.\n",
            "  Batch 1,080  of  2,746.    Elapsed: 0:06:44.\n",
            "  Batch 1,120  of  2,746.    Elapsed: 0:07:00.\n",
            "  Batch 1,160  of  2,746.    Elapsed: 0:07:15.\n",
            "  Batch 1,200  of  2,746.    Elapsed: 0:07:30.\n",
            "  Batch 1,240  of  2,746.    Elapsed: 0:07:45.\n",
            "  Batch 1,280  of  2,746.    Elapsed: 0:08:00.\n",
            "  Batch 1,320  of  2,746.    Elapsed: 0:08:15.\n",
            "  Batch 1,360  of  2,746.    Elapsed: 0:08:30.\n",
            "  Batch 1,400  of  2,746.    Elapsed: 0:08:45.\n",
            "  Batch 1,440  of  2,746.    Elapsed: 0:09:00.\n",
            "  Batch 1,480  of  2,746.    Elapsed: 0:09:15.\n",
            "  Batch 1,520  of  2,746.    Elapsed: 0:09:30.\n",
            "  Batch 1,560  of  2,746.    Elapsed: 0:09:45.\n",
            "  Batch 1,600  of  2,746.    Elapsed: 0:10:00.\n",
            "  Batch 1,640  of  2,746.    Elapsed: 0:10:15.\n",
            "  Batch 1,680  of  2,746.    Elapsed: 0:10:30.\n",
            "  Batch 1,720  of  2,746.    Elapsed: 0:10:45.\n",
            "  Batch 1,760  of  2,746.    Elapsed: 0:11:00.\n",
            "  Batch 1,800  of  2,746.    Elapsed: 0:11:15.\n",
            "  Batch 1,840  of  2,746.    Elapsed: 0:11:30.\n",
            "  Batch 1,880  of  2,746.    Elapsed: 0:11:45.\n",
            "  Batch 1,920  of  2,746.    Elapsed: 0:12:00.\n",
            "  Batch 1,960  of  2,746.    Elapsed: 0:12:15.\n",
            "  Batch 2,000  of  2,746.    Elapsed: 0:12:30.\n",
            "  Batch 2,040  of  2,746.    Elapsed: 0:12:45.\n",
            "  Batch 2,080  of  2,746.    Elapsed: 0:13:00.\n",
            "  Batch 2,120  of  2,746.    Elapsed: 0:13:15.\n",
            "  Batch 2,160  of  2,746.    Elapsed: 0:13:30.\n",
            "  Batch 2,200  of  2,746.    Elapsed: 0:13:45.\n",
            "  Batch 2,240  of  2,746.    Elapsed: 0:14:00.\n",
            "  Batch 2,280  of  2,746.    Elapsed: 0:14:15.\n",
            "  Batch 2,320  of  2,746.    Elapsed: 0:14:30.\n",
            "  Batch 2,360  of  2,746.    Elapsed: 0:14:45.\n",
            "  Batch 2,400  of  2,746.    Elapsed: 0:15:00.\n",
            "  Batch 2,440  of  2,746.    Elapsed: 0:15:15.\n",
            "  Batch 2,480  of  2,746.    Elapsed: 0:15:30.\n",
            "  Batch 2,520  of  2,746.    Elapsed: 0:15:45.\n",
            "  Batch 2,560  of  2,746.    Elapsed: 0:16:00.\n",
            "  Batch 2,600  of  2,746.    Elapsed: 0:16:15.\n",
            "  Batch 2,640  of  2,746.    Elapsed: 0:16:30.\n",
            "  Batch 2,680  of  2,746.    Elapsed: 0:16:45.\n",
            "  Batch 2,720  of  2,746.    Elapsed: 0:17:00.\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:17:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation Loss: 0.21\n",
            "  Validation took: 0:00:38\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  2,746.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  2,746.    Elapsed: 0:00:30.\n",
            "  Batch   120  of  2,746.    Elapsed: 0:00:45.\n",
            "  Batch   160  of  2,746.    Elapsed: 0:01:00.\n",
            "  Batch   200  of  2,746.    Elapsed: 0:01:15.\n",
            "  Batch   240  of  2,746.    Elapsed: 0:01:30.\n",
            "  Batch   280  of  2,746.    Elapsed: 0:01:45.\n",
            "  Batch   320  of  2,746.    Elapsed: 0:02:00.\n",
            "  Batch   360  of  2,746.    Elapsed: 0:02:15.\n",
            "  Batch   400  of  2,746.    Elapsed: 0:02:30.\n",
            "  Batch   440  of  2,746.    Elapsed: 0:02:45.\n",
            "  Batch   480  of  2,746.    Elapsed: 0:03:00.\n",
            "  Batch   520  of  2,746.    Elapsed: 0:03:15.\n",
            "  Batch   560  of  2,746.    Elapsed: 0:03:30.\n",
            "  Batch   600  of  2,746.    Elapsed: 0:03:45.\n",
            "  Batch   640  of  2,746.    Elapsed: 0:04:00.\n",
            "  Batch   680  of  2,746.    Elapsed: 0:04:15.\n",
            "  Batch   720  of  2,746.    Elapsed: 0:04:30.\n",
            "  Batch   760  of  2,746.    Elapsed: 0:04:45.\n",
            "  Batch   800  of  2,746.    Elapsed: 0:05:00.\n",
            "  Batch   840  of  2,746.    Elapsed: 0:05:15.\n",
            "  Batch   880  of  2,746.    Elapsed: 0:05:30.\n",
            "  Batch   920  of  2,746.    Elapsed: 0:05:45.\n",
            "  Batch   960  of  2,746.    Elapsed: 0:06:00.\n",
            "  Batch 1,000  of  2,746.    Elapsed: 0:06:15.\n",
            "  Batch 1,040  of  2,746.    Elapsed: 0:06:31.\n",
            "  Batch 1,080  of  2,746.    Elapsed: 0:06:45.\n",
            "  Batch 1,120  of  2,746.    Elapsed: 0:07:00.\n",
            "  Batch 1,160  of  2,746.    Elapsed: 0:07:16.\n",
            "  Batch 1,200  of  2,746.    Elapsed: 0:07:31.\n",
            "  Batch 1,240  of  2,746.    Elapsed: 0:07:46.\n",
            "  Batch 1,280  of  2,746.    Elapsed: 0:08:01.\n",
            "  Batch 1,320  of  2,746.    Elapsed: 0:08:16.\n",
            "  Batch 1,360  of  2,746.    Elapsed: 0:08:31.\n",
            "  Batch 1,400  of  2,746.    Elapsed: 0:08:46.\n",
            "  Batch 1,440  of  2,746.    Elapsed: 0:09:01.\n",
            "  Batch 1,480  of  2,746.    Elapsed: 0:09:16.\n",
            "  Batch 1,520  of  2,746.    Elapsed: 0:09:31.\n",
            "  Batch 1,560  of  2,746.    Elapsed: 0:09:46.\n",
            "  Batch 1,600  of  2,746.    Elapsed: 0:10:01.\n",
            "  Batch 1,640  of  2,746.    Elapsed: 0:10:16.\n",
            "  Batch 1,680  of  2,746.    Elapsed: 0:10:31.\n",
            "  Batch 1,720  of  2,746.    Elapsed: 0:10:46.\n",
            "  Batch 1,760  of  2,746.    Elapsed: 0:11:01.\n",
            "  Batch 1,800  of  2,746.    Elapsed: 0:11:16.\n",
            "  Batch 1,840  of  2,746.    Elapsed: 0:11:31.\n",
            "  Batch 1,880  of  2,746.    Elapsed: 0:11:46.\n",
            "  Batch 1,920  of  2,746.    Elapsed: 0:12:01.\n",
            "  Batch 1,960  of  2,746.    Elapsed: 0:12:16.\n",
            "  Batch 2,000  of  2,746.    Elapsed: 0:12:31.\n",
            "  Batch 2,040  of  2,746.    Elapsed: 0:12:46.\n",
            "  Batch 2,080  of  2,746.    Elapsed: 0:13:01.\n",
            "  Batch 2,120  of  2,746.    Elapsed: 0:13:16.\n",
            "  Batch 2,160  of  2,746.    Elapsed: 0:13:31.\n",
            "  Batch 2,200  of  2,746.    Elapsed: 0:13:46.\n",
            "  Batch 2,240  of  2,746.    Elapsed: 0:14:01.\n",
            "  Batch 2,280  of  2,746.    Elapsed: 0:14:16.\n",
            "  Batch 2,320  of  2,746.    Elapsed: 0:14:31.\n",
            "  Batch 2,360  of  2,746.    Elapsed: 0:14:46.\n",
            "  Batch 2,400  of  2,746.    Elapsed: 0:15:01.\n",
            "  Batch 2,440  of  2,746.    Elapsed: 0:15:16.\n",
            "  Batch 2,480  of  2,746.    Elapsed: 0:15:31.\n",
            "  Batch 2,520  of  2,746.    Elapsed: 0:15:46.\n",
            "  Batch 2,560  of  2,746.    Elapsed: 0:16:01.\n",
            "  Batch 2,600  of  2,746.    Elapsed: 0:16:16.\n",
            "  Batch 2,640  of  2,746.    Elapsed: 0:16:31.\n",
            "  Batch 2,680  of  2,746.    Elapsed: 0:16:46.\n",
            "  Batch 2,720  of  2,746.    Elapsed: 0:17:01.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:17:11\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.94\n",
            "  Validation Loss: 0.21\n",
            "  Validation took: 0:00:38\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  2,746.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  2,746.    Elapsed: 0:00:30.\n",
            "  Batch   120  of  2,746.    Elapsed: 0:00:45.\n",
            "  Batch   160  of  2,746.    Elapsed: 0:01:00.\n",
            "  Batch   200  of  2,746.    Elapsed: 0:01:15.\n",
            "  Batch   240  of  2,746.    Elapsed: 0:01:30.\n",
            "  Batch   280  of  2,746.    Elapsed: 0:01:45.\n",
            "  Batch   320  of  2,746.    Elapsed: 0:02:00.\n",
            "  Batch   360  of  2,746.    Elapsed: 0:02:15.\n",
            "  Batch   400  of  2,746.    Elapsed: 0:02:30.\n",
            "  Batch   440  of  2,746.    Elapsed: 0:02:45.\n",
            "  Batch   480  of  2,746.    Elapsed: 0:03:00.\n",
            "  Batch   520  of  2,746.    Elapsed: 0:03:15.\n",
            "  Batch   560  of  2,746.    Elapsed: 0:03:30.\n",
            "  Batch   600  of  2,746.    Elapsed: 0:03:45.\n",
            "  Batch   640  of  2,746.    Elapsed: 0:04:00.\n",
            "  Batch   680  of  2,746.    Elapsed: 0:04:15.\n",
            "  Batch   720  of  2,746.    Elapsed: 0:04:30.\n",
            "  Batch   760  of  2,746.    Elapsed: 0:04:45.\n",
            "  Batch   800  of  2,746.    Elapsed: 0:05:00.\n",
            "  Batch   840  of  2,746.    Elapsed: 0:05:15.\n",
            "  Batch   880  of  2,746.    Elapsed: 0:05:30.\n",
            "  Batch   920  of  2,746.    Elapsed: 0:05:45.\n",
            "  Batch   960  of  2,746.    Elapsed: 0:06:00.\n",
            "  Batch 1,000  of  2,746.    Elapsed: 0:06:15.\n",
            "  Batch 1,040  of  2,746.    Elapsed: 0:06:30.\n",
            "  Batch 1,080  of  2,746.    Elapsed: 0:06:45.\n",
            "  Batch 1,120  of  2,746.    Elapsed: 0:07:00.\n",
            "  Batch 1,160  of  2,746.    Elapsed: 0:07:15.\n",
            "  Batch 1,200  of  2,746.    Elapsed: 0:07:30.\n",
            "  Batch 1,240  of  2,746.    Elapsed: 0:07:45.\n",
            "  Batch 1,280  of  2,746.    Elapsed: 0:08:00.\n",
            "  Batch 1,320  of  2,746.    Elapsed: 0:08:15.\n",
            "  Batch 1,360  of  2,746.    Elapsed: 0:08:30.\n",
            "  Batch 1,400  of  2,746.    Elapsed: 0:08:46.\n",
            "  Batch 1,440  of  2,746.    Elapsed: 0:09:01.\n",
            "  Batch 1,480  of  2,746.    Elapsed: 0:09:16.\n",
            "  Batch 1,520  of  2,746.    Elapsed: 0:09:31.\n",
            "  Batch 1,560  of  2,746.    Elapsed: 0:09:46.\n",
            "  Batch 1,600  of  2,746.    Elapsed: 0:10:01.\n",
            "  Batch 1,640  of  2,746.    Elapsed: 0:10:16.\n",
            "  Batch 1,680  of  2,746.    Elapsed: 0:10:31.\n",
            "  Batch 1,720  of  2,746.    Elapsed: 0:10:46.\n",
            "  Batch 1,760  of  2,746.    Elapsed: 0:11:01.\n",
            "  Batch 1,800  of  2,746.    Elapsed: 0:11:16.\n",
            "  Batch 1,840  of  2,746.    Elapsed: 0:11:31.\n",
            "  Batch 1,880  of  2,746.    Elapsed: 0:11:46.\n",
            "  Batch 1,920  of  2,746.    Elapsed: 0:12:01.\n",
            "  Batch 1,960  of  2,746.    Elapsed: 0:12:16.\n",
            "  Batch 2,000  of  2,746.    Elapsed: 0:12:31.\n",
            "  Batch 2,040  of  2,746.    Elapsed: 0:12:46.\n",
            "  Batch 2,080  of  2,746.    Elapsed: 0:13:01.\n",
            "  Batch 2,120  of  2,746.    Elapsed: 0:13:16.\n",
            "  Batch 2,160  of  2,746.    Elapsed: 0:13:31.\n",
            "  Batch 2,200  of  2,746.    Elapsed: 0:13:46.\n",
            "  Batch 2,240  of  2,746.    Elapsed: 0:14:01.\n",
            "  Batch 2,280  of  2,746.    Elapsed: 0:14:16.\n",
            "  Batch 2,320  of  2,746.    Elapsed: 0:14:31.\n",
            "  Batch 2,360  of  2,746.    Elapsed: 0:14:46.\n",
            "  Batch 2,400  of  2,746.    Elapsed: 0:15:01.\n",
            "  Batch 2,440  of  2,746.    Elapsed: 0:15:16.\n",
            "  Batch 2,480  of  2,746.    Elapsed: 0:15:31.\n",
            "  Batch 2,520  of  2,746.    Elapsed: 0:15:46.\n",
            "  Batch 2,560  of  2,746.    Elapsed: 0:16:01.\n",
            "  Batch 2,600  of  2,746.    Elapsed: 0:16:16.\n",
            "  Batch 2,640  of  2,746.    Elapsed: 0:16:31.\n",
            "  Batch 2,680  of  2,746.    Elapsed: 0:16:46.\n",
            "  Batch 2,720  of  2,746.    Elapsed: 0:17:01.\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:17:11\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.94\n",
            "  Validation Loss: 0.24\n",
            "  Validation took: 0:00:38\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  2,746.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  2,746.    Elapsed: 0:00:30.\n",
            "  Batch   120  of  2,746.    Elapsed: 0:00:45.\n",
            "  Batch   160  of  2,746.    Elapsed: 0:01:00.\n",
            "  Batch   200  of  2,746.    Elapsed: 0:01:15.\n",
            "  Batch   240  of  2,746.    Elapsed: 0:01:30.\n",
            "  Batch   280  of  2,746.    Elapsed: 0:01:45.\n",
            "  Batch   320  of  2,746.    Elapsed: 0:02:00.\n",
            "  Batch   360  of  2,746.    Elapsed: 0:02:15.\n",
            "  Batch   400  of  2,746.    Elapsed: 0:02:30.\n",
            "  Batch   440  of  2,746.    Elapsed: 0:02:45.\n",
            "  Batch   480  of  2,746.    Elapsed: 0:03:00.\n",
            "  Batch   520  of  2,746.    Elapsed: 0:03:15.\n",
            "  Batch   560  of  2,746.    Elapsed: 0:03:30.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}